<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Lukas Kuhn</title>

  <meta name="author" content="Lukas Kuhn">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Lukas Kuhn
                  </p>
                  <p>
                    I'm a research scientist at the <a href="https://mlo-lab.github.io/">MLO Lab</a> lead by Prof. Dr.
                    Florian Buettner in Frankfurt and a Masters student at Goethe University Frankfurt in AI &
                    Computational Neuroscience.

                  </p>
                  <p style="text-align:center">
                    <a href="mailto:lukas.kuhn@dkfz-heidelberg.De">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=-Y-DZ1AAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/lukas-kuhn/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/me.jpg"><img style="width:100%;max-width:100%;aspect-ratio: 1/1; border-radius: 50%;"
                      alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I study how systems can learn structured predictive models of the world, drawing on energy-based
                    modeling, generative AI, and insights from neuroscience.
                    Some papers are <span class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/nova.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2602.00653">
                    <span class="papertitle">Non-Contrastive Vision-Language Learning with Predictive Embedding
                      Alignment</span>
                  </a>
                  <br>
                  <strong>Lukas Kuhn</strong>,
                  <span>Giuseppe Serra</span>
                  <span>Florian Buettner</span>
                  <br>
                  <em>arXiv</em>, 2026
                  <br>
                  <a href="https://arxiv.org/abs/2602.00653">arXiv</a>
                  <p></p>
                  <p>
                    We introduce NOVA, a NOn-contrastive Vision-language Alignment framework
                    based on joint embedding prediction with distributional regularization. NOVA aligns visual
                    representations to a frozen, domain-specific text encoder by predicting text embeddings from
                    augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic
                    Gaussian Regularization (SIGReg).
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/lvlmva.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2512.21985">
                    <span class="papertitle">LVLM-Aided Alignment of Task-Specific Vision Models</span>
                  </a>
                  <br>
                  <span>Alexander Koeber</span>,
                  <strong>Lukas Kuhn</strong>,
                  <span>Ingo Thon</span>
                  <span>Florian Buettner</span>
                  <br>
                  <em>arXiv</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2512.21985">arXiv</a>
                  <p></p>
                  <p>
                    Small vision models in high-stakes domains often learn spurious correlations that don't align with
                    human expertise, leading to unreliable real-world performance. LVLM-VA addresses this by using large
                    vision-language models as a bridge between domain experts and task-specific models, translating
                    human knowledge into actionable feedback that reduces reliance on spurious features without
                    requiring fine-grained annotations.
                  </p>
                </td>
              </tr>


              <tr bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/asm.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://lukas-kuhn.github.io/shortcut-detection-mitigation-page/">
                    <span class="papertitle">Efficient unsupervised shortcut learning detection and mitigation in
                      transformers</span>
                  </a>
                  <br>
                  <strong>Lukas Kuhn</strong>,
                  <span>Sari Sadiya</span>,
                  <span>Joerg Schlotterer</span>,
                  <span>Florian Buettner</span>,
                  <span>Christin Seifert</span>,
                  <span>Gemma Roig</span>
                  <br>
                  <em>ICCV</em>, 2025
                  <br>
                  <a href="https://lukas-kuhn.github.io/shortcut-detection-mitigation-page/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2501.00942">arXiv</a>
                  <p></p>
                  <p>
                    By leveraging MLLMs and the representational structure of the Transformer architecture we are able
                    to detect and mitigate shortcuts completely unsupervised.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/modelauditorworkflow.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2507.05755">
                    <span class="papertitle">An autonomous agent for auditing and improving the reliability of clinical
                      AI models</span>
                  </a>
                  <br>
                  <strong>Lukas Kuhn</strong>,
                  <span>Florian Buettner</span>
                  <br>
                  <em>MICCAI MedAgent</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2501.00942">arXiv</a>
                  <p></p>
                  <p>
                    Multi-agent architecture that generates interpretable reports explaining how much computer vision
                    model performance likely degrades during deployment, discussing specific likely failure modes and
                    identifying root causes and mitigation strategies.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/beyondcalibration.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2506.09593">
                    <span class="papertitle">Beyond Overconfidence: Foundation Models Redefine Calibration in Deep
                      Neural Networks</span>
                  </a>
                  <br>
                  <span>Achim Heckler</span>,
                  <strong>Lukas Kuhn</strong>,
                  <span>Florian Buettner</span>
                  <br>
                  <em>arXiv</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2506.09593">arXiv</a>
                  <p></p>
                  <p>
                    Empirical analysis of vision foundation models that shows that these models tend to be
                    underconfident in
                    in-distribution predictions, resulting in higher calibration errors, while demonstrating improved
                    calibration under distribution shifts.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/realignmethod.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2506.09593">
                    <span class="papertitle">Cognitive Neural Architecture Search Reveals Hierarchical Entailment</span>
                  </a>
                  <br>
                  <strong>Lukas Kuhn</strong>,
                  <span>Sari Sadiya</span>,
                  <span>Gemma Roig</span>
                  <br>
                  <em>ICLR ReAlign</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2502.11141">arXiv</a>
                  <p></p>
                  <p>
                    We demonstrate that optimizing convolutional network architectures for brain-alignment via
                    evolutionary neural architecture search results in models with clear representational hierarchies.
                    The identified models achieve brain-alignment scores surpassing even those of pretrained
                    classification models.
                  </p>
                </td>
              </tr>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <br>
                      <p style="text-align:right;font-size:small;">
                        Website Design copied with permission from <a
                          href="https://jonbarron.github.io/">https://jonbarron.github.io/</a>
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
        </td>
      </tr>
  </table>
</body>

</html>